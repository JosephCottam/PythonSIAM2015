{
 "metadata": {
  "name": "",
  "signature": "sha256:fc038d92ac149c5cb4b353289cebce0143e01f9e0f49ab8ec1e7bb81093316dc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"images/continuum_analytics_logo.png\" \n",
      "                                alt=\"Continuum Logo\",\n",
      "                                align=\"right\",\n",
      "                                width=\"30%\">,\n",
      "\n",
      "\n",
      "Remote Data\n",
      "===========\n",
      "\n",
      "We've seen how `into` and `blaze` provide intuitive access to various forms of local data and computation.  Now we extend these to large remote systems.\n",
      "\n",
      "* `into` will help us move data between local, remote, and HDFS locations as well as help us to register that data with the Hive metastore\n",
      "* `blaze` will help us query those same computational systems and bring results back for local analysis\n",
      "\n",
      "\n",
      "<img src=\"images/hdfs.png\" align=\"right\" width=\"50%\">\n",
      "\n",
      "Cluster on EC2\n",
      "--------------\n",
      "\n",
      "You have access to a small cluster on EC2.  It consists of a few individual machines connected with HDFS.  The cluster also runs various computational services like Hive, Impala, and Spark.\n",
      "\n",
      "Data can live in a few places\n",
      "\n",
      "* Locally (`myfile.csv`) on the notebook computer in front of you\n",
      "* Remotely (`ssh://hostname:myfile.csv`) on the file system of one of the remote machines \n",
      "* On HDFS (`hdfs://hostname:myfile.csv`) on the parallel file system shared by all of the machines\n",
      "* Registered in the Hive Metastore (`hive://hostname::tablename`) as a SQL table\n",
      "\n",
      "Traditionally we interact with each of these systems through different means.  Individually each provides a simple interface but taken as a collection the burden on new developers can be significant.\n",
      "\n",
      "Hostnames:\n",
      "\n",
      "* `54.159.103.12` - Hive, Spark, HDFS\n",
      "* `54.145.126.122` - Impala"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "auth = {\n",
      "  'username': '',\n",
      "  'password': ''\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`into`\n",
      "------\n",
      "\n",
      "We connect to the individual machines through `ssh` using the [`paramiko`](http://www.paramiko.org/) library and to HDFS as a whole through WebHDFS using [`pywebhdfs`](https://pypi.python.org/pypi/pywebhdfs).\n",
      "\n",
      "    conda install paramiko\n",
      "    pip install pywebhdfs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We pull down a remote csv file into a Pandas DataFrame and push it up to HDFS as a JSON file.\n",
      "\n",
      "*Warning:  You are all on the same cluster sharing the same resources.  You are likely to overwrite each others' work.  Nothing that you put on the cluster is secure.*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from into import into\n",
      "\n",
      "# Load data from ssh://54.159.103.12:data/iris.csv into a DataFrame\n",
      "# Use the auth dictionary above to supply authentication\n",
      "\n",
      "into(..., ..., **auth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Modify that data somehow (or just leave it alone) and upload your results to HDFS \n",
      "# hdfs://54.159.103.12:your-filename.json\n",
      "\n",
      "into(..., ..., **auth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`blaze`\n",
      "-------\n",
      "\n",
      "We now interact with common HDFS database technologies.  Part of this involves using the Hive Metastore.  Hive has some overhead and was not really intended for rapid, interactive feedback.  If we all start hammering the Hive server it might become unhappy.  The tutorial leader should have done one of the following:\n",
      "\n",
      "```python\n",
      "into('hive://hostname/default::tablename', 'local-file.csv')\n",
      "into('hive://hostname/default::tablename', 'ssh://hostname:remote-file.csv')\n",
      "into('hive://hostname/default::tablename', 'hdfs://hostname:remote-directory/*.csv')\n",
      "```\n",
      "\n",
      "You now have the opportunity to play with this data (and others) using your choice of Database.\n",
      "\n",
      "You should quickly verify that you have the following libraries installed\n",
      "\n",
      "* For Impala: `impyla`\n",
      "\n",
      "        pip install -U impyla \n",
      "\n",
      "* For Hive: `pyhive`\n",
      "\n",
      "        conda install -c blaze pyhive  # Mac/Linux\n",
      "        Instructions at https://github.com/dropbox/PyHive  # Windows\n",
      "    \n",
      "* For Redshift: `redshift-sqlalchemy`\n",
      "\n",
      "        pip install redshift-sqlalchemy\n",
      "\n",
      "These are trivially pip installable.  (TODO: check this)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Loading data into Hive\n",
      "\n",
      "We uploaded the NYCTaxiCab dataset to Hive in the following way \n",
      "\n",
      "(Please, don't everyone do this)\n",
      "\n",
      "```python\n",
      "In [1]: from into import into\n",
      "\n",
      "In [2]: into('hive://54.159.103.12::taxi', 'ssh://54.159.103.12:/mnt/all.csv', **auth)\n",
      "             \n",
      "INFO:paramiko.transport:Connected (version 2.0, client OpenSSH_6.6.1p1)\n",
      "INFO:paramiko.transport:Authentication (publickey) failed.\n",
      "INFO:paramiko.transport:Authentication (publickey) failed.\n",
      "INFO:paramiko.transport:Authentication (publickey) failed.\n",
      "INFO:paramiko.transport:Authentication (publickey) failed.\n",
      "INFO:paramiko.transport:Authentication (password) successful!\n",
      "INFO:paramiko.transport.sftp:[chan 0] Opened sftp connection (server version 3)\n",
      "INFO:pyhive.hive:USE `default`\n",
      "INFO:pyhive.hive:SHOW TABLES\n",
      "INFO:pyhive.hive:DESCRIBE foo\n",
      "INFO:pyhive.hive:DESCRIBE foo\n",
      "INFO:pyhive.hive:        CREATE  TABLE default.taxi (\n",
      "            medallion  STRING,\n",
      "            hack_license  STRING,\n",
      "               vendor_id  STRING,\n",
      "               rate_code  BIGINT,\n",
      "      store_and_fwd_flag  STRING,\n",
      "         pickup_datetime  TIMESTAMP,\n",
      "        dropoff_datetime  TIMESTAMP,\n",
      "         passenger_count  BIGINT,\n",
      "       trip_time_in_secs  BIGINT,\n",
      "           trip_distance  DOUBLE,\n",
      "        pickup_longitude  DOUBLE,\n",
      "         pickup_latitude  DOUBLE,\n",
      "       dropoff_longitude  DOUBLE,\n",
      "        dropoff_latitude  DOUBLE,\n",
      "            tolls_amount  DOUBLE,\n",
      "              tip_amount  DOUBLE,\n",
      "            total_amount  DOUBLE,\n",
      "                 mta_tax  DOUBLE,\n",
      "             fare_amount  DOUBLE,\n",
      "            payment_type  STRING,\n",
      "               surcharge  DOUBLE\n",
      "        )\n",
      "        ROW FORMAT DELIMITED\n",
      "            FIELDS TERMINATED BY ','\n",
      "        STORED AS TEXTFILE\n",
      "        \n",
      "        TBLPROPERTIES (\"skip.header.line.count\"=\"1\")\n",
      "INFO:pyhive.hive:DESCRIBE taxi\n",
      "INFO:pyhive.hive:DESCRIBE taxi\n",
      "INFO:pyhive.hive:LOAD DATA LOCAL INPATH \"/mnt/all.csv\" INTO TABLE taxi\n",
      "\n",
      "Out[2]: Table('taxi', MetaData(bind=Engine(hive://hdfs@54.159.103.12:10000/default)), Column('medallion', String(), table=<taxi>), Column('hack_license', String(), table=<taxi>), Column('vendor_id', String(), table=<taxi>), Column('rate_code', BigInteger(), table=<taxi>), Column('store_and_fwd_flag', String(), table=<taxi>), Column('pickup_datetime', HiveTimestamp(), table=<taxi>), Column('dropoff_datetime', HiveTimestamp(), table=<taxi>), Column('passenger_count', BigInteger(), table=<taxi>), Column('trip_time_in_secs', BigInteger(), table=<taxi>), Column('trip_distance', Float(), table=<taxi>), Column('pickup_longitude', Float(), table=<taxi>), Column('pickup_latitude', Float(), table=<taxi>), Column('dropoff_longitude', Float(), table=<taxi>), Column('dropoff_latitude', Float(), table=<taxi>), Column('tolls_amount', Float(), table=<taxi>), Column('tip_amount', Float(), table=<taxi>), Column('total_amount', Float(), table=<taxi>), Column('mta_tax', Float(), table=<taxi>), Column('fare_amount', Float(), table=<taxi>), Column('payment_type', String(), table=<taxi>), Column('surcharge', Float(), table=<taxi>), schema=None)\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Hive - needs PyHive  (conda install -c blaze PyHive # if Linux/Max)\n",
      "import blaze as bz\n",
      "\n",
      "hive = bz.Data('hive://54.159.103.12')\n",
      "hive.dshape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Impala - needs impyla  (pip install -U impyla)\n",
      "imp = bz.Data('impala://54.145.126.122')\n",
      "imp.dshape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Postgres - needs psycopg2  (conda install psycopg2)\n",
      "pg = bz.Data('postgresql://postgres:postgres@ec2-54-159-160-163.compute-1.amazonaws.com')\n",
      "pg.dshape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interact with the rest of the ecosystem\n",
      "---------------------------------------\n",
      "\n",
      "Because Blaze expressions implement standard Python protocols like `__iter__` and `__array__` they can sometimes interact with the rest of the ecosystem without thought."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "taxi = imp.taxi[(imp.taxi.trip_distance > 0.1) & (imp.taxi.trip_distance < 100)]\n",
      "\n",
      "query = bz.by(taxi.medallion, avg=taxi.trip_distance.mean())\n",
      "\n",
      "plt.semilogy(query.avg.sort())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
